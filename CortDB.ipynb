{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Загрузка пакетов и данных"
      ],
      "metadata": {
        "id": "-w8m7BHlUL-s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1swNa8oJxPCJ"
      },
      "outputs": [],
      "source": [
        "!pip install biopython matplotlib seaborn pandas numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# загрузка рабочих директорий\n",
        "from Bio import SeqIO\n",
        "import os\n",
        "import gzip\n",
        "from google.colab import drive\n",
        "\n",
        "# Подключение к Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Путь к рабочей папке\n",
        "work_dir = \"/content/drive/MyDrive/Colab Notebooks/IT_expedition/CortDB/\"\n",
        "os.chdir(work_dir)\n"
      ],
      "metadata": {
        "id": "uucUbELKu4Zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# сверка файлов и таблицы\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Пути\n",
        "work_dir = \"/content/drive/MyDrive/Colab Notebooks/IT_expedition/CortDB/\"\n",
        "rawseq_dir = os.path.join(work_dir, \"rawseq\")\n",
        "sequences_file = os.path.join(work_dir, \"sequences.csv\")\n",
        "\n",
        "# 1. Получаем список файлов в rawseq (без путей, только имена)\n",
        "if not os.path.exists(rawseq_dir):\n",
        "    raise FileNotFoundError(f\"Папка 'rawseq' не найдена: {rawseq_dir}\")\n",
        "\n",
        "files_in_rawseq = set(os.listdir(rawseq_dir))\n",
        "\n",
        "# 2. Загружаем и фильтруем данные из sequences.csv\n",
        "if not os.path.exists(sequences_file):\n",
        "    raise FileNotFoundError(f\"Файл 'sequences.csv' не найден: {sequences_file}\")\n",
        "\n",
        "df = pd.read_csv(sequences_file)\n",
        "\n",
        "# Фильтруем: исключаем строки, где seqResult == \"contam\"\n",
        "valid_sequences = df[df[\"seqResult\"] != \"contam\"]\n",
        "trace_files_in_csv = set(valid_sequences[\"traceFileName\"].dropna().unique())\n",
        "\n",
        "# 3. Сравниваем\n",
        "only_in_rawseq = files_in_rawseq - trace_files_in_csv\n",
        "only_in_csv = trace_files_in_csv - files_in_rawseq\n",
        "\n",
        "# 4. Выводим результаты\n",
        "print(\"Файлы, которые есть в rawseq, но отсутствуют в sequences.csv (исключая 'contam'):\")\n",
        "print(only_in_rawseq)\n",
        "\n",
        "print(\"\\nФайлы, которые есть в sequences.csv (исключая 'contam'), но отсутствуют в rawseq:\")\n",
        "print(only_in_csv)\n",
        "\n",
        "# Проверка на полное совпадение\n",
        "if not only_in_rawseq and not only_in_csv:\n",
        "    print(\"\\n✅ Все имена файлов совпадают (с учётом фильтрации)!\")\n",
        "else:\n",
        "    print(\"\\n❌ Есть расхождения (см. выше).\")\n",
        "\n",
        "# (Опционально) Сохранение отчёта в файл\n",
        "report = [\n",
        "    \"Отчёт о сравнении файлов:\",\n",
        "    \"---\",\n",
        "    \"Файлы только в rawseq:\",\n",
        "    *only_in_rawseq,\n",
        "    \"\",\n",
        "    \"Файлы только в sequences.csv (исключая 'contam'):\",\n",
        "    *only_in_csv,\n",
        "    \"\",\n",
        "    f\"Совпадающих файлов: {len(files_in_rawseq & trace_files_in_csv)}\",\n",
        "    f\"Файлов только в rawseq: {len(only_in_rawseq)}\",\n",
        "    f\"Файлов только в sequences.csv: {len(only_in_csv)}\"\n",
        "]\n",
        "\n",
        "with open(\"file_comparison_report.txt\", \"w\") as f:\n",
        "    f.write(\"\\n\".join(report))"
      ],
      "metadata": {
        "id": "5ZfXaeKmIjlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Конвертация исходных файлов и анализ качества"
      ],
      "metadata": {
        "id": "7opXAUZ9USb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Конвертация с сохранением полного имени и упаковкой в .gz\n",
        "\n",
        "import os\n",
        "import gzip\n",
        "from Bio import SeqIO\n",
        "\n",
        "# Указываем рабочую директорию с .ab1 файлами\n",
        "work_dir = \"/content/drive/MyDrive/Colab Notebooks/IT_expedition/CortDB/rawseq\"\n",
        "os.chdir(work_dir)  # Переходим в нужную папку\n",
        "\n",
        "print(f\"Текущая директория: {os.getcwd()}\")\n",
        "print(f\"Файлы в директории: {os.listdir()}\")\n",
        "\n",
        "# Конвертация .ab1 в .fastq.gz\n",
        "for filename in os.listdir(work_dir):\n",
        "    if filename.endswith(\".ab1\"):\n",
        "        fastq_name = filename.replace(\".ab1\", \".fastq\")\n",
        "        gz_name = fastq_name + \".gz\"\n",
        "        ab1_path = os.path.join(work_dir, filename)  # Полный путь к .ab1 файлу\n",
        "\n",
        "        print(f\"Обработка файла: {filename}\")\n",
        "\n",
        "        try:\n",
        "            with gzip.open(gz_name, \"wt\") as gz_handle:\n",
        "                for record in SeqIO.parse(ab1_path, \"abi\"):\n",
        "                    record.id = filename.replace(\".ab1\", \"\")\n",
        "                    record.description = \"\"\n",
        "                    SeqIO.write(record, gz_handle, \"fastq\")\n",
        "            print(f\"Успешно конвертирован: {filename} → {gz_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка при обработке {filename}: {str(e)}\")\n",
        "\n",
        "# Проверка результатов\n",
        "print(\"\\nРезультаты конвертации:\")\n",
        "fastq_gz_files = [f for f in os.listdir() if f.endswith('.fastq.gz')]\n",
        "if fastq_gz_files:\n",
        "    print(\"Найдены сжатые FASTQ файлы:\")\n",
        "    for f in fastq_gz_files:\n",
        "        print(f)\n",
        "    print(\"\\nПример содержимого первого файла:\")\n",
        "    !zcat {fastq_gz_files[0]} | head -n 4\n",
        "\n",
        "    # Упаковываем в ZIP для скачивания\n",
        "    !zip -r converted_fastq_gz.zip *.fastq.gz\n",
        "    print(\"\\nСкачать архив:\")\n",
        "    from google.colab import files\n",
        "    files.download(\"converted_fastq_gz.zip\")\n",
        "else:\n",
        "    print(\"Не найдено ни одного .fastq.gz файла. Возможные причины:\")\n",
        "    print(\"1. В директории нет .ab1 файлов\")\n",
        "    print(\"2. Возникли ошибки при конвертации\")\n",
        "    print(f\"Проверьте содержимое папки: {work_dir}\")\n",
        "    print(\"Содержимое папки:\")\n",
        "    !ls -la"
      ],
      "metadata": {
        "id": "Uh4fRwx_xznL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# оценка качества сиквенсов\n",
        "import pandas as pd\n",
        "from Bio import SeqIO\n",
        "import gzip\n",
        "import os\n",
        "\n",
        "# Путь к папке с .fastq.gz файлами\n",
        "work_dir = \"/content/drive/MyDrive/Colab Notebooks/IT_expedition/CortDB/rawseq\"\n",
        "os.chdir(work_dir)\n",
        "\n",
        "# Сбор данных по ВСЕМ ридам\n",
        "all_data = []\n",
        "for filename in os.listdir():\n",
        "    if filename.endswith(\".fastq.gz\"):\n",
        "        with gzip.open(filename, \"rt\") as handle:\n",
        "            for record in SeqIO.parse(handle, \"fastq\"):\n",
        "                mean_q = sum(record.letter_annotations[\"phred_quality\"]) / len(record.seq)\n",
        "                all_data.append({\n",
        "                    \"file\": filename,\n",
        "                    \"sequence_id\": record.id,\n",
        "                    \"length\": len(record.seq),\n",
        "                    \"mean_quality\": mean_q\n",
        "                })\n",
        "\n",
        "# Создание DataFrame\n",
        "df = pd.DataFrame(all_data)\n",
        "\n",
        "# Сохранение данных о качестве (отдельный файл)\n",
        "df.to_csv(\"all_sequences_quality.csv\", index=False)\n",
        "print(\"Данные о качестве последовательностей сохранены в all_sequences_quality.csv\")\n",
        "\n",
        "# Загрузка метаданных\n",
        "metadata_path = \"/content/drive/MyDrive/Colab Notebooks/IT_expedition/CortDB/sequences.csv\"\n",
        "metadata_df = pd.read_csv(metadata_path)\n",
        "\n",
        "# Создаем ключ для слияния - удаляем расширения\n",
        "df['merge_key'] = df['file'].str.replace('.fastq.gz', '')\n",
        "metadata_df['merge_key'] = metadata_df['traceFileName'].str.replace('.ab1', '')\n",
        "\n",
        "# Добавляем колонки о качестве в метаданные\n",
        "for col in ['length', 'mean_quality']:\n",
        "    if col in metadata_df.columns:\n",
        "        metadata_df.drop(col, axis=1, inplace=True)  # Удаляем если уже есть\n",
        "\n",
        "# Слияние данных\n",
        "metadata_df = metadata_df.merge(\n",
        "    df[['merge_key', 'length', 'mean_quality']],\n",
        "    on='merge_key',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Обновляем поле seqResult\n",
        "metadata_df['seqResult'] = metadata_df.apply(\n",
        "    lambda row: 'seq failed' if pd.notna(row['mean_quality']) and row['mean_quality'] < 20 else row['seqResult'],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Удаляем временный столбец merge_key\n",
        "metadata_df.drop('merge_key', axis=1, inplace=True)\n",
        "\n",
        "# Перезаписываем исходный файл метаданных (сначала делаем backup)\n",
        "backup_path = metadata_path.replace('.csv', '_backup.csv')\n",
        "metadata_df.to_csv(backup_path, index=False)\n",
        "print(f\"\\nСоздана резервная копия метаданных: {backup_path}\")\n",
        "\n",
        "metadata_df.to_csv(metadata_path, index=False)\n",
        "print(f\"Исходные метаданные обновлены: {metadata_path}\")\n",
        "\n",
        "# Проверка\n",
        "print(\"\\nПервые 5 строк обновленных метаданных:\")\n",
        "display(metadata_df.head())"
      ],
      "metadata": {
        "id": "3mgG8vMFxTAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# визуализация качества исходных сиквенсов\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Patch\n",
        "import numpy as np\n",
        "import os\n",
        "from Bio import SeqIO\n",
        "import gzip\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Убедитесь, что df и low_q_df определены перед использованием\n",
        "# df = ... ваш код загрузки данных ...\n",
        "# low_q_df = df[df[\"mean_quality\"] < 20]\n",
        "\n",
        "# Создаем фигуру с тремя областями графиков\n",
        "plt.figure(figsize=(16, 12))\n",
        "\n",
        "# =============================================\n",
        "# График 1: Распределение качества (верхний левый)\n",
        "# =============================================\n",
        "plt.subplot(2, 2, 1)\n",
        "\n",
        "# Гистограмма качества\n",
        "n_qual, bins_qual, patches_qual = plt.hist(df[\"mean_quality\"], bins=50, color=\"steelblue\")\n",
        "\n",
        "# Закрашиваем столбцы с Q < 20\n",
        "for patch, bin_edge in zip(patches_qual, bins_qual):\n",
        "    if bin_edge < 20:\n",
        "        patch.set_facecolor('firebrick')\n",
        "    else:\n",
        "        patch.set_facecolor('steelblue')\n",
        "\n",
        "plt.axvline(x=20, color=\"black\", linestyle=\"--\", linewidth=1.5)\n",
        "plt.xlabel(\"Средний Phred score качества\", fontsize=12)\n",
        "plt.ylabel(\"Количество ридов\", fontsize=12)\n",
        "plt.title(\"Распределение качества ридов\", fontsize=14, pad=20)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Легенда для качества\n",
        "legend_elements_qual = [\n",
        "    Patch(facecolor='steelblue', label='Q ≥ 20 (хорошее)'),\n",
        "    Patch(facecolor='firebrick', label='Q < 20 (низкое)'),\n",
        "    Patch(facecolor='none', edgecolor='black', linestyle='--',\n",
        "          label='Порог качества (Q=20)')\n",
        "]\n",
        "plt.legend(handles=legend_elements_qual, fontsize=10)\n",
        "\n",
        "# =============================================\n",
        "# График 2: Распределение длин (верхний правый)\n",
        "# =============================================\n",
        "plt.subplot(2, 2, 2)\n",
        "\n",
        "# Автоматическое определение разумных бинов для длины\n",
        "len_bins = np.linspace(df[\"length\"].min(), df[\"length\"].max(), 50)\n",
        "\n",
        "# Гистограмма длин\n",
        "n_len, bins_len, patches_len = plt.hist(df[\"length\"], bins=len_bins, color=\"forestgreen\")\n",
        "\n",
        "# Вычисляем 5-й и 95-й перцентили для выделения\n",
        "len_5th = df[\"length\"].quantile(0.05)\n",
        "len_95th = df[\"length\"].quantile(0.95)\n",
        "\n",
        "# Закрашиваем выбросы\n",
        "for patch, bin_edge in zip(patches_len, bins_len):\n",
        "    if bin_edge < len_5th or bin_edge > len_95th:\n",
        "        patch.set_facecolor('goldenrod')\n",
        "    else:\n",
        "        patch.set_facecolor('forestgreen')\n",
        "\n",
        "plt.axvline(x=len_5th, color=\"black\", linestyle=\":\", linewidth=1)\n",
        "plt.axvline(x=len_95th, color=\"black\", linestyle=\":\", linewidth=1)\n",
        "plt.xlabel(\"Длина рида (нуклеотиды)\", fontsize=12)\n",
        "plt.ylabel(\"Количество ридов\", fontsize=12)\n",
        "plt.title(\"Распределение длин ридов\", fontsize=14, pad=20)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Легенда для длин\n",
        "legend_elements_len = [\n",
        "    Patch(facecolor='forestgreen', label='Основной диапазон'),\n",
        "    Patch(facecolor='goldenrod', label='5%/95% перцентили'),\n",
        "    Patch(facecolor='none', edgecolor='black', linestyle=':',\n",
        "          label='Границы перцентилей')\n",
        "]\n",
        "plt.legend(handles=legend_elements_len, fontsize=10)\n",
        "\n",
        "# =============================================\n",
        "# График 3: Профиль качества (нижний, на всю ширину)\n",
        "# =============================================\n",
        "plt.subplot(2, 1, 2)\n",
        "\n",
        "# Определяем путь к директории с fastq файлами\n",
        "work_dir = \"/content/drive/MyDrive/Colab Notebooks/IT_expedition/CortDB/rawseq\"\n",
        "\n",
        "# Сбор данных о качестве\n",
        "quality_profiles = []\n",
        "max_observed_q = 0\n",
        "\n",
        "for filename in tqdm([f for f in os.listdir(work_dir) if f.endswith(\".fastq.gz\")],\n",
        "                   desc=\"Анализ качества\"):\n",
        "    with gzip.open(os.path.join(work_dir, filename), \"rt\") as handle:\n",
        "        for record in SeqIO.parse(handle, \"fastq\"):\n",
        "            quals = record.letter_annotations[\"phred_quality\"]\n",
        "            quality_profiles.append(quals)\n",
        "            current_max = max(quals)\n",
        "            if current_max > max_observed_q:\n",
        "                max_observed_q = current_max\n",
        "\n",
        "# Определение параметров графика\n",
        "y_max = max(60, max_observed_q + 5)  # Минимум до 60, с запасом +5\n",
        "x_max = max(len(q) for q in quality_profiles)\n",
        "\n",
        "# Усреднение по позициям (с обработкой разных длин)\n",
        "avg_quality = []\n",
        "for pos in range(x_max):\n",
        "    pos_qualities = [q[pos] for q in quality_profiles if pos < len(q)]\n",
        "    avg_quality.append(np.mean(pos_qualities) if pos_qualities else np.nan)\n",
        "\n",
        "# Построение графика\n",
        "plt.plot(avg_quality, color='royalblue', linewidth=2.5, label='Среднее качество')\n",
        "\n",
        "# Горизонтальные линии порогов\n",
        "thresholds = {\n",
        "    'Отлично (Q≥40)': {'value': 40, 'color': 'darkgreen', 'linestyle': '--'},\n",
        "    'Хорошо (Q≥30)': {'value': 30, 'color': 'limegreen', 'linestyle': ':'},\n",
        "    'Приемлемо (Q≥20)': {'value': 20, 'color': 'gold', 'linestyle': '-.'},\n",
        "    'Плохо (Q<20)': {'value': 10, 'color': 'red', 'linestyle': '--'}\n",
        "}\n",
        "\n",
        "for label, params in thresholds.items():\n",
        "    plt.axhline(y=params['value'], color=params['color'],\n",
        "               linestyle=params['linestyle'], alpha=0.7, label=label)\n",
        "\n",
        "# Настройки графика\n",
        "plt.title('Профиль качества сиквенсов', fontsize=16, pad=20)\n",
        "plt.xlabel('Позиция в риде', fontsize=14)\n",
        "plt.ylabel('Phred score', fontsize=14)\n",
        "plt.ylim(0, y_max)\n",
        "plt.xlim(0, x_max)\n",
        "\n",
        "# Легенда и сетка\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.grid(alpha=0.2)\n",
        "\n",
        "# Добавление аннотации с максимальным качеством\n",
        "plt.annotate(f'Макс. качество: Q{max_observed_q}',\n",
        "            xy=(0.02, 0.95), xycoords='axes fraction',\n",
        "            fontsize=12, bbox=dict(boxstyle='round', alpha=0.1))\n",
        "\n",
        "# =============================================\n",
        "# Общие настройки\n",
        "# =============================================\n",
        "plt.tight_layout(pad=3.0)\n",
        "\n",
        "# Добавляем общий заголовок\n",
        "plt.suptitle(\"Анализ качества и длины последовательностей\",\n",
        "             fontsize=16, y=1.02)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Вывод статистики\n",
        "print(\"\\nСтатистика по качеству:\")\n",
        "print(f\"• Среднее качество: {df['mean_quality'].mean():.1f}\")\n",
        "print(f\"• Доля ридов с Q < 20: {len(low_q_df)/len(df)*100:.1f}%\")\n",
        "\n",
        "print(\"\\nСтатистика по длинам:\")\n",
        "print(f\"• Средняя длина: {df['length'].mean():.1f} нт\")\n",
        "print(f\"• 5-й перцентиль: {len_5th:.1f} нт\")\n",
        "print(f\"• 95-й перцентиль: {len_95th:.1f} нт\")\n",
        "\n",
        "print(f\"\\nСтатистика профиля качества:\\nМаксимальное качество: Q{max_observed_q}\\n\"\n",
        "      f\"Среднее качество: {np.nanmean(avg_quality):.1f}\\n\"\n",
        "      f\"Длина ридов: {x_max} п.н.\")"
      ],
      "metadata": {
        "id": "9idsQdG7aQyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Обработка сиквенсов"
      ],
      "metadata": {
        "id": "mVa7jP_VUbWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Обработка сиквенсов (обрезка, слияние) и экспорт всех фаста файлов по отдельности и вместе и запись последовательностей в таблицу метаданных\n",
        "\n",
        "import os\n",
        "import gzip\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from Bio import SeqIO, pairwise2\n",
        "from Bio.Seq import Seq\n",
        "from Bio.SeqRecord import SeqRecord\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from google.colab import files as colab_files\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "# ======================\n",
        "# КОНФИГУРАЦИЯ\n",
        "# ======================\n",
        "QUALITY_PROFILES = {\n",
        "    'strict': {\n",
        "        'min_length': 50,\n",
        "        'min_avg_quality': 20,\n",
        "        'min_end_quality': 15,\n",
        "        'min_overlap': 20,\n",
        "        'window_size': 5\n",
        "    },\n",
        "    'moderate': {\n",
        "        'min_length': 40,\n",
        "        'min_avg_quality': 15,\n",
        "        'min_end_quality': 10,\n",
        "        'min_overlap': 15,\n",
        "        'window_size': 5\n",
        "    },\n",
        "    'loose': {\n",
        "        'min_length': 30,\n",
        "        'min_avg_quality': 10,\n",
        "        'min_end_quality': 7,\n",
        "        'min_overlap': 10,\n",
        "        'window_size': 5\n",
        "    }\n",
        "}\n",
        "\n",
        "CURRENT_PROFILE = QUALITY_PROFILES['strict']\n",
        "\n",
        "input_dir = \"/content/drive/MyDrive/Colab Notebooks/IT_expedition/CortDB/rawseq\"\n",
        "output_dir = \"/content/drive/MyDrive/Colab Notebooks/IT_expedition/CortDB/\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "original_dir = \"/content/drive/MyDrive/Colab Notebooks/IT_expedition/CortDB\"\n",
        "\n",
        "# ======================\n",
        "# ОСНОВНЫЕ ФУНКЦИИ\n",
        "# ======================\n",
        "def load_filtered_metadata(metadata_path):\n",
        "    \"\"\"Загружает и фильтрует метаданные по seqResult\"\"\"\n",
        "    df = pd.read_csv(metadata_path)\n",
        "    print(f\"Всего записей в метаданных: {len(df)}\")\n",
        "\n",
        "def load_filtered_metadata(metadata_path):\n",
        "    \"\"\"Загружает и фильтрует метаданные по seqResult и pcrResult\"\"\"\n",
        "    df = pd.read_csv(metadata_path)\n",
        "    print(f\"Всего записей в метаданных: {len(df)}\")\n",
        "\n",
        "    # Фильтрация по seqResult и pcrResult\n",
        "    filtered_df = df[\n",
        "        (~df['seqResult'].isin(['seq failed', 'pcr failed']))\n",
        "    ]\n",
        "    print(f\"Осталось после фильтрации: {len(filtered_df)} (исключены seq failed и pcr failed)\")\n",
        "\n",
        "    return filtered_df\n",
        "\n",
        "def calculate_sequence_quality(sequence):\n",
        "    \"\"\"Вычисляет характеристики качества последовательности\"\"\"\n",
        "    if not sequence or not isinstance(sequence, str):\n",
        "        return {\n",
        "            'seq_length': 0,\n",
        "            'n_count': 0,\n",
        "            'n_percentage': 100.0,\n",
        "            'quality_score': 0.0\n",
        "        }\n",
        "\n",
        "    seq_length = len(sequence)\n",
        "    n_count = sequence.upper().count('N')\n",
        "    n_percentage = (n_count / seq_length) * 100 if seq_length > 0 else 100.0\n",
        "    quality_score = (seq_length * 0.4) - (n_percentage * 0.6)\n",
        "\n",
        "    return {\n",
        "        'seq_length': seq_length,\n",
        "        'n_count': n_count,\n",
        "        'n_percentage': round(n_percentage, 2),\n",
        "        'quality_score': round(quality_score, 2)\n",
        "    }\n",
        "\n",
        "def process_sanger_read(filepath, params):\n",
        "    \"\"\"Обработка рида с учетом параметров качества\"\"\"\n",
        "    try:\n",
        "        with gzip.open(filepath, 'rt') as f:\n",
        "            for record in SeqIO.parse(f, 'fastq'):\n",
        "                quals = record.letter_annotations[\"phred_quality\"]\n",
        "\n",
        "                # Проверка среднего качества\n",
        "                if np.mean(quals) < params['min_avg_quality']:\n",
        "                    continue\n",
        "\n",
        "                # Определение границ хорошего качества\n",
        "                start = next((i for i in range(len(quals)-params['window_size'])\n",
        "                            if np.mean(quals[i:i+params['window_size']]) >= params['min_end_quality']), 0)\n",
        "                end = next((i for i in range(len(quals)-1, params['window_size'], -1)\n",
        "                          if np.mean(quals[i-params['window_size']:i]) >= params['min_end_quality']), len(quals))\n",
        "\n",
        "                # Обрезка и проверка длины\n",
        "                trimmed = record[start:end]\n",
        "                if len(trimmed) >= params['min_length']:\n",
        "                    return trimmed\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка обработки {filepath}: {str(e)}\")\n",
        "    return None\n",
        "\n",
        "def assemble_contigs(f_read, r_read, sample_id, params):\n",
        "    \"\"\"Собирает контиги из прямого и обратного ридов\"\"\"\n",
        "    try:\n",
        "        f_seq = str(f_read.seq)\n",
        "        r_seq = str(r_read.reverse_complement().seq)\n",
        "\n",
        "        # Выравнивание последовательностей\n",
        "        alignments = pairwise2.align.globalms(\n",
        "            f_seq, r_seq, 2, -3, -5, -2, one_alignment_only=True\n",
        "        )\n",
        "\n",
        "        if not alignments:\n",
        "            return None\n",
        "\n",
        "        # Построение консенсусной последовательности\n",
        "        aligned_f, aligned_r = alignments[0][0], alignments[0][1]\n",
        "        consensus = []\n",
        "        overlap_len = 0\n",
        "\n",
        "        for f_base, r_base in zip(aligned_f, aligned_r):\n",
        "            if f_base == '-' or r_base == '-':\n",
        "                continue\n",
        "            overlap_len += 1\n",
        "            consensus.append(f_base if f_base == r_base else f_base)\n",
        "\n",
        "        # Проверка минимального перекрытия\n",
        "        if overlap_len >= params['min_overlap']:\n",
        "            return SeqRecord(\n",
        "                Seq(''.join(consensus)),\n",
        "                id=f\"contig_{sample_id}\",\n",
        "                description=f\"len={len(consensus)}\"\n",
        "            )\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка сборки {sample_id}: {str(e)}\")\n",
        "    return None\n",
        "\n",
        "def process_sample(sample_id, f_files, r_files, params, df_meta):\n",
        "    \"\"\"Обрабатывает один образец\"\"\"\n",
        "    # Получаем ручную последовательность (если есть)\n",
        "    manual_seq = df_meta[df_meta['catNumber'] == sample_id]['Sequence'].iloc[0] if not df_meta[df_meta['catNumber'] == sample_id].empty else None\n",
        "\n",
        "    # Получаем лучшие риды для каждого направления\n",
        "    best_f = None\n",
        "    best_r = None\n",
        "\n",
        "    for file in f_files:\n",
        "        read = process_sanger_read(os.path.join(input_dir, file.replace('.ab1', '.fastq.gz')), params)\n",
        "        if read and (best_f is None or np.mean(read.letter_annotations[\"phred_quality\"]) >\n",
        "                     np.mean(best_f.letter_annotations[\"phred_quality\"])):\n",
        "            best_f = read\n",
        "\n",
        "    for file in r_files:\n",
        "        read = process_sanger_read(os.path.join(input_dir, file.replace('.ab1', '.fastq.gz')), params)\n",
        "        if read and (best_r is None or np.mean(read.letter_annotations[\"phred_quality\"]) >\n",
        "                     np.mean(best_r.letter_annotations[\"phred_quality\"])):\n",
        "            best_r = read\n",
        "\n",
        "    # Сборка контига или использование одиночных ридов\n",
        "    result = {\n",
        "        'sample_id': sample_id,\n",
        "        'result_type': 'failed',\n",
        "        'auto_sequence': None,\n",
        "        'identity_percent': None,\n",
        "        'sequence_preview': None\n",
        "    }\n",
        "\n",
        "    if best_f and best_r:\n",
        "        contig = assemble_contigs(best_f, best_r, sample_id, params)\n",
        "        if contig:\n",
        "            result['auto_sequence'] = str(contig.seq)\n",
        "            result['result_type'] = 'contig'\n",
        "            SeqIO.write(contig, os.path.join(output_dir, f\"{sample_id}_contig.fasta\"), \"fasta\")\n",
        "\n",
        "    if not result['auto_sequence']:\n",
        "        if best_f:\n",
        "            result['auto_sequence'] = str(best_f.seq)\n",
        "            result['result_type'] = 'F_only'\n",
        "            SeqIO.write(best_f, os.path.join(output_dir, f\"{sample_id}_F.fasta\"), \"fasta\")\n",
        "        elif best_r:\n",
        "            result['auto_sequence'] = str(best_r.seq)\n",
        "            result['result_type'] = 'R_only'\n",
        "            SeqIO.write(best_r, os.path.join(output_dir, f\"{sample_id}_R.fasta\"), \"fasta\")\n",
        "\n",
        "    # Сравнение с ручной последовательностью\n",
        "    if result['auto_sequence'] and manual_seq:\n",
        "        identity, preview = compare_sequences(result['auto_sequence'], manual_seq)\n",
        "        result['identity_percent'] = identity\n",
        "        result['sequence_preview'] = preview\n",
        "\n",
        "    # Расчет характеристик качества\n",
        "    quality_metrics = calculate_sequence_quality(result['auto_sequence'])\n",
        "    result.update(quality_metrics)\n",
        "\n",
        "    return result\n",
        "\n",
        "def compare_sequences(auto_seq, manual_seq):\n",
        "    \"\"\"Сравнение последовательностей и расчет идентичности\"\"\"\n",
        "    if pd.isna(manual_seq) or not manual_seq or not isinstance(manual_seq, str):\n",
        "        return None, None\n",
        "\n",
        "    if not auto_seq:\n",
        "        return 0.0, None\n",
        "\n",
        "    manual_seq = manual_seq.upper().replace(' ', '').replace('\\n', '')\n",
        "    auto_seq = auto_seq.upper().replace(' ', '').replace('\\n', '')\n",
        "\n",
        "    if not manual_seq or not auto_seq:\n",
        "        return 0.0, None\n",
        "\n",
        "    aligner = pairwise2.align.globalxx(auto_seq, manual_seq)\n",
        "    if not aligner:\n",
        "        return 0.0, None\n",
        "\n",
        "    best_aln = aligner[0]\n",
        "    matches = sum(a == b for a, b in zip(best_aln[0], best_aln[1]))\n",
        "    identity = matches / max(len(best_aln[0]), len(best_aln[1]))\n",
        "\n",
        "    preview = f\"Auto: {auto_seq[:50]}...\\nManual: {manual_seq[:50]}...\"\n",
        "    return round(identity * 100, 2), preview\n",
        "\n",
        "def main(metadata_path):\n",
        "    \"\"\"Основная функция обработки\"\"\"\n",
        "    # Загрузка и фильтрация метаданных\n",
        "    df_meta = load_filtered_metadata(metadata_path)\n",
        "\n",
        "    # Группировка файлов по образцам и направлениям\n",
        "    samples = defaultdict(lambda: {'F': [], 'R': []})\n",
        "\n",
        "    for _, row in df_meta.iterrows():\n",
        "        # Проверяем наличие направления и catNumber\n",
        "        if pd.isna(row['direction']) or pd.isna(row['catNumber']):\n",
        "            print(f\"Пропуск записи с пустым direction или catNumber: {row}\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            samples[row['catNumber']][row['direction']].append(row['traceFileName'])\n",
        "        except KeyError as e:\n",
        "            print(f\"Ошибка обработки строки: {row}\")\n",
        "            print(f\"Некорректное значение направления: {row['direction']}\")\n",
        "            continue\n",
        "\n",
        "    # Остальной код остается без изменений...\n",
        "    # Обработка каждого образца\n",
        "    results = []\n",
        "    all_sequences = []\n",
        "\n",
        "    for sample_id, files in tqdm(samples.items(), desc=\"Обработка образцов\"):\n",
        "        result = process_sample(sample_id, files['F'], files['R'], CURRENT_PROFILE, df_meta)\n",
        "        results.append(result)\n",
        "\n",
        "        if result['auto_sequence']:\n",
        "            record = SeqRecord(\n",
        "                Seq(result['auto_sequence']),\n",
        "                id=f\"{sample_id}_{result['result_type']}\",\n",
        "                description=f\"identity={result['identity_percent']}%\"\n",
        "            )\n",
        "            all_sequences.append(record)\n",
        "\n",
        "    # Сохранение результатов\n",
        "    save_results(results, all_sequences, df_meta)\n",
        "def save_results(results, all_sequences, original_meta):\n",
        "    \"\"\"Сохраняет все результаты обработки\"\"\"\n",
        "    # Сохранение объединенного FASTA\n",
        "    combined_fasta = os.path.join(output_dir, \"all_sequences.fasta\")\n",
        "    with open(combined_fasta, \"w\") as output_handle:\n",
        "        SeqIO.write(all_sequences, output_handle, \"fasta\")\n",
        "\n",
        "    # Создание итоговой таблицы\n",
        "    df_results = pd.DataFrame(results)\n",
        "    df_final = pd.merge(\n",
        "        original_meta,\n",
        "        df_results,\n",
        "        left_on='catNumber',\n",
        "        right_on='sample_id',\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    # Категоризация качества\n",
        "    df_final['quality_category'] = pd.cut(\n",
        "        df_final['quality_score'],\n",
        "        bins=[-float('inf'), 0, 150, 200, 250, float('inf')],\n",
        "        labels=['Failed', 'Low', 'Medium', 'Good', 'Excellent']\n",
        "    )\n",
        "\n",
        "    # Сохранение CSV\n",
        "    output_csv = os.path.join(output_dir, \"comparison_results_with_quality.csv\")\n",
        "    original_output_csv = os.path.join(original_dir, \"comparison_results_with_quality.csv\")\n",
        "    df_final.to_csv(output_csv, index=False)\n",
        "    df_final.to_csv(original_output_csv, index=False)\n",
        "\n",
        "    # Архивирование\n",
        "    shutil.make_archive(\"sanger_results\", 'zip', output_dir)\n",
        "    colab_files.download(\"sanger_results.zip\")\n",
        "\n",
        "    # Вывод статистики\n",
        "    print_statistics(df_final)\n",
        "\n",
        "def print_statistics(df):\n",
        "    \"\"\"Выводит статистику обработки\"\"\"\n",
        "    print(\"\\n=== СТАТИСТИКА ОБРАБОТКИ ===\")\n",
        "    print(f\"Всего образцов: {len(df)}\")\n",
        "    print(f\"Успешно собрано контигов: {len(df[df['result_type'] == 'contig'])}\")\n",
        "    print(f\"Только прямой рид: {len(df[df['result_type'] == 'F_only'])}\")\n",
        "    print(f\"Только обратный рид: {len(df[df['result_type'] == 'R_only'])}\")\n",
        "    print(f\"Не удалось обработать: {len(df[df['result_type'] == 'failed'])}\")\n",
        "\n",
        "    print(\"\\n=== КАЧЕСТВО ПОСЛЕДОВАТЕЛЬНОСТЕЙ ===\")\n",
        "    print(f\"Средняя длина: {df['seq_length'].mean():.1f} bp\")\n",
        "    print(f\"Среднее количество N: {df['n_count'].mean():.1f}\")\n",
        "    print(f\"Средний процент N: {df['n_percentage'].mean():.1f}%\")\n",
        "    print(\"\\nРаспределение по категориям качества:\")\n",
        "    print(df['quality_category'].value_counts())\n",
        "\n",
        "    if 'identity_percent' in df.columns:\n",
        "        valid_comparisons = df['identity_percent'].notna()\n",
        "        print(f\"\\nСредняя идентичность с ручными данными: {df[valid_comparisons]['identity_percent'].mean():.1f}%\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    metadata_path = \"/content/drive/MyDrive/Colab Notebooks/IT_expedition/CortDB/sequences.csv\"\n",
        "    main(metadata_path)"
      ],
      "metadata": {
        "id": "0lu3EDeyjdZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# визуализация качества полученных фаста файлов\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from time import time\n",
        "\n",
        "# Конфигурация\n",
        "INPUT_CSV = \"/content/drive/MyDrive/Colab Notebooks/IT_expedition/CortDB/comparison_results_with_quality.csv\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/Colab Notebooks/IT_expedition/CortDB/quality_plots/\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "start_time = time()\n",
        "\n",
        "# 1. Загрузка данных\n",
        "df = pd.read_csv(INPUT_CSV, usecols=[\n",
        "    'seq_length', 'n_percentage',\n",
        "    'quality_score', 'quality_category',\n",
        "    'result_type'\n",
        "])\n",
        "\n",
        "# 2. Настройка стиля\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "plt.figure(figsize=(16, 12))\n",
        "\n",
        "# --- ВЕРХНИЙ РЯД: ГИСТОГРАММЫ ---\n",
        "\n",
        "# График 1: Распределение длин\n",
        "plt.subplot(2, 2, 1)\n",
        "sns.histplot(df['seq_length'], bins=30, kde=False, color='royalblue')\n",
        "plt.title(f\"Распределение длин последовательностей\\n(Среднее: {df['seq_length'].mean():.1f} bp)\")\n",
        "plt.xlabel(\"Длина (bp)\")\n",
        "plt.ylabel(\"Количество\")\n",
        "\n",
        "# График 2: Распределение Quality Score\n",
        "plt.subplot(2, 2, 2)\n",
        "sns.histplot(df['quality_score'], bins=30, kde=True, color='purple')\n",
        "plt.title(f\"Распределение Quality Score\\n(Среднее: {df['quality_score'].mean():.1f})\")\n",
        "plt.xlabel(\"Quality Score\")\n",
        "plt.ylabel(\"Частота\")\n",
        "\n",
        "# --- НИЖНИЙ РЯД: КРУГОВЫЕ ДИАГРАММЫ ---\n",
        "\n",
        "# График 3: Категории качества\n",
        "plt.subplot(2, 2, 3)\n",
        "quality_counts = df['quality_category'].value_counts()\n",
        "plt.pie(quality_counts,\n",
        "        labels=quality_counts.index,\n",
        "        autopct='%1.1f%%',\n",
        "        startangle=90,\n",
        "        colors=sns.color_palette(\"husl\", len(quality_counts)),\n",
        "        textprops={'fontsize': 10})\n",
        "plt.title(\"Категории качества\", pad=20)\n",
        "\n",
        "# График 4: Типы результатов сборки\n",
        "plt.subplot(2, 2, 4)\n",
        "if 'result_type' in df.columns:\n",
        "    result_counts = df['result_type'].value_counts()\n",
        "    plt.pie(result_counts,\n",
        "            labels=result_counts.index,\n",
        "            autopct='%1.1f%%',\n",
        "            startangle=90,\n",
        "            colors=sns.color_palette(\"Set2\", len(result_counts)),\n",
        "            textprops={'fontsize': 10})\n",
        "    plt.title(\"Типы результатов сборки\", pad=20)\n",
        "else:\n",
        "    plt.text(0.5, 0.5, 'Данные о типах сборки отсутствуют',\n",
        "             ha='center', va='center', fontsize=10)\n",
        "    plt.title(\"Типы сборки недоступны\", pad=20)\n",
        "\n",
        "# Общие настройки\n",
        "plt.tight_layout(pad=3.0)\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"final_quality_metrics.png\"),\n",
        "            dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# 3. Генерация отчета\n",
        "execution_time = time() - start_time\n",
        "stats_report = f\"\"\"\n",
        "=== ОТЧЕТ ПО КАЧЕСТВУ ===\n",
        "Время генерации: {execution_time:.2f} сек\n",
        "Всего образцов: {len(df)}\n",
        "--- Основные метрики ---\n",
        "Средняя длина: {df['seq_length'].mean():.1f} ± {df['seq_length'].std():.1f} bp\n",
        "Средний процент N: {df['n_percentage'].mean():.2f}% ± {df['n_percentage'].std():.2f}%\n",
        "Средний Quality Score: {df['quality_score'].mean():.2f} ± {df['quality_score'].std():.2f}\n",
        "--- Распределение по категориям ---\n",
        "{df['quality_category'].value_counts().to_string()}\n",
        "\"\"\"\n",
        "\n",
        "print(stats_report)\n",
        "with open(os.path.join(OUTPUT_DIR, \"quality_report.txt\"), 'w') as f:\n",
        "    f.write(stats_report)"
      ],
      "metadata": {
        "id": "fVEBKT7Fv8aQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Работа с данными классификации BLASTN"
      ],
      "metadata": {
        "id": "YYg4sR30UlfL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# парсинг html файла результатов BLASTN из PlutoF|UNITE\n",
        "\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "\n",
        "def parse_blastn_html(html_file_path):\n",
        "    try:\n",
        "        with open(html_file_path, 'r', encoding='utf-8') as f:\n",
        "            soup = BeautifulSoup(f, 'html.parser')\n",
        "\n",
        "        queries = []\n",
        "\n",
        "        # Находим все блоки с запросами\n",
        "        query_blocks = soup.find_all('div', style=lambda x: x and 'margin-top: 20px' in x)\n",
        "\n",
        "        for block in query_blocks:\n",
        "            # Извлекаем информацию о запросе\n",
        "            query_text = block.get_text(strip=True)\n",
        "            if 'Query' in query_text:\n",
        "                query_num = query_text.split('Query')[1].split('of')[0].strip()\n",
        "                query_name = query_text.split(':')[1].split('identity=')[0].strip()\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            # Находим следующую таблицу после блока запроса\n",
        "            table = block.find_next('table')\n",
        "            if not table:\n",
        "                continue\n",
        "\n",
        "            hits = []\n",
        "            # Обрабатываем строки таблицы (пропускаем заголовок)\n",
        "            for row in table.find_all('tr')[1:]:\n",
        "                cols = row.find_all('td')\n",
        "                if len(cols) >= 13:  # Проверяем, что есть все необходимые колонки\n",
        "                    hit = {\n",
        "                        'Reference': cols[0].get_text(strip=True),\n",
        "                        'SH:0.5%': cols[1].get_text(strip=True),\n",
        "                        'SH:1.0%': cols[2].get_text(strip=True),\n",
        "                        'SH:1.5%': cols[3].get_text(strip=True),\n",
        "                        'Taxon name': cols[4].get_text(strip=True),\n",
        "                        'Score': cols[5].get_text(strip=True),\n",
        "                        'E-value': cols[6].get_text(strip=True),\n",
        "                        'Prcnt': cols[7].get_text(strip=True),\n",
        "                        'MisM': cols[8].get_text(strip=True),\n",
        "                        'Qstart': cols[9].get_text(strip=True),\n",
        "                        'Qend': cols[10].get_text(strip=True),\n",
        "                        'Rstart': cols[11].get_text(strip=True),\n",
        "                        'Rend': cols[12].get_text(strip=True)\n",
        "                    }\n",
        "                    hits.append(hit)\n",
        "\n",
        "            if hits:\n",
        "                queries.append({\n",
        "                    'query_num': query_num,\n",
        "                    'query_name': query_name,\n",
        "                    'hits': hits\n",
        "                })\n",
        "\n",
        "        # Преобразуем в плоскую таблицу\n",
        "        rows = []\n",
        "        for query in queries:\n",
        "            for hit in query['hits']:\n",
        "                row = {\n",
        "                    'Query': query['query_name'],\n",
        "                    'Reference': hit['Reference'],\n",
        "                    'SH:0.5%': hit['SH:0.5%'],\n",
        "                    'SH:1.0%': hit['SH:1.0%'],\n",
        "                    'SH:1.5%': hit['SH:1.5%'],\n",
        "                    'Taxon name': hit['Taxon name'],\n",
        "                    'Score': hit['Score'],\n",
        "                    'E-value': hit['E-value'],\n",
        "                    'Prcnt': hit['Prcnt'],\n",
        "                    'MisM': hit['MisM'],\n",
        "                    'Qstart': hit['Qstart'],\n",
        "                    'Qend': hit['Qend'],\n",
        "                    'Rstart': hit['Rstart'],\n",
        "                    'Rend': hit['Rend']\n",
        "                }\n",
        "                rows.append(row)\n",
        "\n",
        "        if not rows:\n",
        "            raise ValueError(\"Не найдено ни одного результата BLAST в файле\")\n",
        "\n",
        "        return pd.DataFrame(rows)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при разборе файла: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Основной код выполнения\n",
        "try:\n",
        "    blast_html_path = \"/content/drive/MyDrive/Colab Notebooks/IT_expedition/CortDB/BlastN.html\"\n",
        "    print(f\"Обработка файла: {blast_html_path}\")\n",
        "\n",
        "    if not os.path.exists(blast_html_path):\n",
        "        raise FileNotFoundError(f\"Файл не найден: {blast_html_path}\")\n",
        "\n",
        "    df = parse_blastn_html(blast_html_path)\n",
        "\n",
        "    # Сохранение результатов\n",
        "    output_csv_path = os.path.join(os.path.dirname(blast_html_path), \"blast_results.csv\")\n",
        "    df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "    print(f\"\\nУспешно обработано:\")\n",
        "    print(f\"Всего запросов: {df['Query'].nunique()}\")\n",
        "    print(f\"Всего результатов: {len(df)}\")\n",
        "    print(f\"CSV сохранен: {output_csv_path}\")\n",
        "\n",
        "    # Проверка первых строк\n",
        "    print(\"\\nПервые 3 строки результатов:\")\n",
        "    print(df.head(3))\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nКритическая ошибка: {str(e)}\")\n",
        "    print(\"Рекомендации:\")\n",
        "    print(\"1. Проверьте формат входного файла\")\n",
        "    print(\"2. Убедитесь, что файл содержит результаты BLAST в HTML-таблицах\")\n",
        "    print(\"3. Если проблема сохраняется, предоставьте полный файл для анализа\")"
      ],
      "metadata": {
        "id": "eMwKoLTx1xes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# выгрузка типовых образцов паутинников из NCBI\n",
        "\n",
        "from Bio import Entrez\n",
        "import time\n",
        "import pandas as pd\n",
        "from urllib.error import HTTPError\n",
        "\n",
        "Entrez.email = \"filippova.courlee.nina@gmail.com\"  # Критически важно указать реальный email\n",
        "NCBI_DELAY = 3  # Задержка между запросами (сек)\n",
        "MAX_RETRIES = 5  # Максимальное число попыток\n",
        "\n",
        "def safe_entrez_request(func, *args, **kwargs):\n",
        "    \"\"\"Обертка для запросов к Entrez с повторами при ошибках\"\"\"\n",
        "    for attempt in range(MAX_RETRIES):\n",
        "        try:\n",
        "            time.sleep(NCBI_DELAY)\n",
        "            return func(*args, **kwargs)\n",
        "        except HTTPError as e:\n",
        "            if e.code == 429:\n",
        "                wait = (attempt + 1) * 10  # Увеличиваем задержку\n",
        "                print(f\"Слишком много запросов. Ждем {wait} сек...\")\n",
        "                time.sleep(wait)\n",
        "            else:\n",
        "                raise\n",
        "    raise Exception(f\"Не удалось выполнить запрос после {MAX_RETRIES} попыток\")\n",
        "\n",
        "# 1. Поиск типовых образцов Cortinarius\n",
        "try:\n",
        "    print(\"Поиск типовых последовательностей...\")\n",
        "    handle = safe_entrez_request(\n",
        "        Entrez.esearch,\n",
        "        db=\"nucleotide\",\n",
        "        term='\"Cortinarius\"[Organism] AND \"type material\"[Filter]',\n",
        "        retmax=1000,\n",
        "        usehistory=\"y\"\n",
        "    )\n",
        "    search_results = Entrez.read(handle)\n",
        "    handle.close()\n",
        "\n",
        "    # Получаем ID через History (для больших наборов)\n",
        "    webenv = search_results[\"WebEnv\"]\n",
        "    query_key = search_results[\"QueryKey\"]\n",
        "    count = int(search_results[\"Count\"])\n",
        "    print(f\"Найдено {count} типовых образцов\")\n",
        "\n",
        "    # 2. Получение метаданных порциями\n",
        "    batch_size = 100\n",
        "    all_records = []\n",
        "\n",
        "    for start in range(0, count, batch_size):\n",
        "        print(f\"Обработка записей {start+1}-{min(start+batch_size, count)}\")\n",
        "\n",
        "        handle = safe_entrez_request(\n",
        "            Entrez.efetch,\n",
        "            db=\"nucleotide\",\n",
        "            rettype=\"gb\",\n",
        "            retmode=\"xml\",\n",
        "            retstart=start,\n",
        "            retmax=batch_size,\n",
        "            webenv=webenv,\n",
        "            query_key=query_key\n",
        "        )\n",
        "        records = Entrez.read(handle)\n",
        "        all_records.extend(records)\n",
        "        handle.close()\n",
        "\n",
        "    # 3. Извлечение accession numbers\n",
        "    type_accessions = []\n",
        "    for record in all_records:\n",
        "        acc = record['GBSeq_primary-accession']\n",
        "        organism = record['GBSeq_organism']\n",
        "\n",
        "        # Проверка типа материала\n",
        "        is_type = False\n",
        "        for feature in record.get('GBSeq_feature-table', []):\n",
        "            if feature.get('GBFeature_key') == 'source':\n",
        "                for qual in feature.get('GBFeature_quals', []):\n",
        "                    if qual.get('GBQualifier_name') == 'type_material':\n",
        "                        is_type = True\n",
        "                        break\n",
        "\n",
        "        if is_type:\n",
        "            type_accessions.append(acc)\n",
        "            print(f\"{acc}: {organism} (ТИПОВОЙ)\")\n",
        "\n",
        "    # 4. Сохранение результатов\n",
        "    df = pd.DataFrame({'Accession': type_accessions})\n",
        "    df.to_csv('Cortinarius_type_materials.csv', index=False)\n",
        "    print(f\"\\nСохранено {len(type_accessions)} типовых accession numbers в Cortinarius_type_materials.csv\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка: {str(e)}\")"
      ],
      "metadata": {
        "id": "i5U15-vD73k5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# выбор лучшей идентицикации (best hit) по приоритету типовой образец > лучшей score\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "# ======================\n",
        "# НАСТРОЙКИ\n",
        "# ======================\n",
        "INPUT_PATH = \"/content/drive/MyDrive/Colab Notebooks/IT_expedition/CortDB/blast_results.csv\"\n",
        "TYPES_DB_PATH = \"Cortinarius_type_materials.csv\"\n",
        "SKIP_PREFIXES = ['UDB', 'MZ', 'KU', 'HQ', 'KM', 'MT', 'MH', 'DQ']\n",
        "\n",
        "# ======================\n",
        "# ФУНКЦИИ\n",
        "# ======================\n",
        "def load_type_database():\n",
        "    \"\"\"Загружает базу типовых образцов и возвращает set\"\"\"\n",
        "    type_db = pd.read_csv(TYPES_DB_PATH)\n",
        "    return set(type_db['Accession'].astype(str).str.strip().values)\n",
        "\n",
        "def is_species_level(taxon_name):\n",
        "    \"\"\"Проверяет, является ли таксон бинарным названием (видом)\"\"\"\n",
        "    if pd.isna(taxon_name):\n",
        "        return False\n",
        "    parts = str(taxon_name).strip().split()\n",
        "    return len(parts) >= 2 and parts[1] not in ['sp.', 'sp', 'cf.', 'aff.']\n",
        "\n",
        "def select_best_hits(df):\n",
        "    \"\"\"\n",
        "    Выбирает лучшие хиты для каждого Query с приоритетом:\n",
        "    1. Типовые образцы с определением до вида\n",
        "    2. Максимальный Score среди видовых определений\n",
        "    \"\"\"\n",
        "    # Фильтруем только видовые определения\n",
        "    species_hits = df[df['Taxon name'].apply(is_species_level)].copy()\n",
        "\n",
        "    # Сортировка по приоритетам\n",
        "    species_hits.sort_values(['Query', 'is_type', 'Score'],\n",
        "                           ascending=[True, False, False], inplace=True)\n",
        "\n",
        "    # Выбор лучшего хита для каждого Query\n",
        "    best_hits = species_hits.groupby('Query').first().reset_index()\n",
        "\n",
        "    # Добавляем список всех уникальных видовых таксонов для каждого Query\n",
        "    unique_taxa = species_hits.groupby('Query')['Taxon name'].unique().apply(\n",
        "        lambda x: '|'.join(sorted(set(x), key=str)))\n",
        "    best_hits['All_Taxa'] = best_hits['Query'].map(unique_taxa)\n",
        "\n",
        "    return best_hits\n",
        "\n",
        "# ======================\n",
        "# ОСНОВНОЙ БЛОК\n",
        "# ======================\n",
        "def main():\n",
        "    try:\n",
        "        # 1. Загрузка данных\n",
        "        print(\"Загрузка данных BLAST...\")\n",
        "        blast_df = pd.read_csv(INPUT_PATH)\n",
        "        print(f\"Загружено строк: {len(blast_df)}\")\n",
        "\n",
        "        # 2. Загрузка базы типовых образцов\n",
        "        print(\"Загрузка базы типовых образцов...\")\n",
        "        type_accessions = load_type_database()\n",
        "        print(f\"Загружено {len(type_accessions)} типовых последовательностей\")\n",
        "\n",
        "        # 3. Фильтрация проблемных accession\n",
        "        blast_df = blast_df[~blast_df['Reference'].astype(str).str.startswith(tuple(SKIP_PREFIXES))]\n",
        "        print(f\"Осталось после фильтрации: {len(blast_df)}\")\n",
        "\n",
        "        # 4. Добавление меток типовых образцов\n",
        "        blast_df['is_type'] = blast_df['Reference'].astype(str).isin(type_accessions)\n",
        "\n",
        "        # 5. Выбор лучших хитов (только видовые определения)\n",
        "        print(\"Выбор лучших хитов (только видовые определения)...\")\n",
        "        best_hits_df = select_best_hits(blast_df)\n",
        "\n",
        "        # 6. Сохранение результатов\n",
        "        output_path = INPUT_PATH.replace(\".csv\", \"_best_species_hits.csv\")\n",
        "        best_hits_df.to_csv(output_path, index=False)\n",
        "\n",
        "        # 7. Статистика\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"ФИНАЛЬНЫЕ РЕЗУЛЬТАТЫ\")\n",
        "        print(f\"Всего запросов: {len(best_hits_df)}\")\n",
        "        print(f\"Найдено типовых образцов: {best_hits_df['is_type'].sum()}\")\n",
        "        print(f\"Уникальных видов: {best_hits_df['All_Taxa'].nunique()}\")\n",
        "        print(f\"Сохранено в: {output_path}\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        # Примеры типовых образцов\n",
        "        type_samples = best_hits_df[best_hits_df['is_type']].head(3)\n",
        "        if not type_samples.empty:\n",
        "            print(\"\\nПримеры типовых образцов (определенных до вида):\")\n",
        "            print(type_samples[['Query', 'Reference', 'Taxon name', 'Score']])\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n!!! КРИТИЧЕСКАЯ ОШИБКА: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "KPLDjAV6GBej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# слияние best hit с исходной таблицей сиквенсов\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Загрузка данных\n",
        "output_dir = \"/content/drive/MyDrive/Colab Notebooks/IT_expedition/CortDB\"  # Замените на ваш путь\n",
        "blast_best_path = os.path.join(output_dir, \"blast_results_best_species_hits.csv\")\n",
        "comparison_path = os.path.join(output_dir, \"comparison_results_with_quality.csv\")\n",
        "\n",
        "# 1. Обработка blast_best_hits.csv - создание catNumber\n",
        "blast_df = pd.read_csv(blast_best_path)\n",
        "\n",
        "# Извлекаем catNumber из Query (все до первого подчеркивания)\n",
        "blast_df['catNumber'] = blast_df['Query'].str.split('_').str[0]\n",
        "\n",
        "# Проверка\n",
        "print(\"Уникальные catNumber в blast_best_hits:\")\n",
        "print(blast_df['catNumber'].unique()[:10])  # Покажем первые 10 для проверки\n",
        "\n",
        "# 2. Загрузка comparison_results.csv\n",
        "comparison_df = pd.read_csv(comparison_path)\n",
        "\n",
        "# Проверка\n",
        "print(\"\\nУникальные catNumber в comparison_results:\")\n",
        "print(comparison_df['catNumber'].unique()[:10])\n",
        "\n",
        "# 3. Объединение таблиц\n",
        "# Левое соединение, чтобы сохранить все строки из comparison_results\n",
        "merged_df = pd.merge(\n",
        "    comparison_df,\n",
        "    blast_df,\n",
        "    on='catNumber',\n",
        "    how='left',\n",
        "    suffixes=('', '_blast')\n",
        ")\n",
        "\n",
        "# 4. Сохранение результата\n",
        "merged_path = os.path.join(output_dir, \"final_merged_results.csv\")\n",
        "merged_df.to_csv(merged_path, index=False)\n",
        "\n",
        "print(f\"\\nОбъединение завершено. Результат сохранен в: {merged_path}\")\n",
        "print(f\"Исходные размеры: comparison_results - {comparison_df.shape}, blast_best_hits - {blast_df.shape}\")\n",
        "print(f\"Итоговый размер: {merged_df.shape}\")"
      ],
      "metadata": {
        "id": "sKN3DgtoKtkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# сравнение машинной и ручной идентификации\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from difflib import SequenceMatcher\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Настройки путей\n",
        "INPUT_PATH = \"/content/drive/MyDrive/Colab Notebooks/IT_expedition/CortDB/final_merged_results.csv\"\n",
        "OUTPUT_PATH = INPUT_PATH.replace(\".csv\", \"_with_comparison_final_v2.csv\")\n",
        "\n",
        "def normalize_name(name):\n",
        "    \"\"\"Нормализация названий таксонов (без рода и окончаний)\"\"\"\n",
        "    if pd.isna(name):\n",
        "        return ''\n",
        "    name = str(name).lower().strip()\n",
        "    name = re.sub(r'\\s+', ' ', name)\n",
        "    name = re.sub(r'[^a-z ]', '', name)\n",
        "    return name\n",
        "\n",
        "def extract_species(name):\n",
        "    \"\"\"Извлечение видового эпитета (второго слова)\"\"\"\n",
        "    if pd.isna(name):\n",
        "        return ''\n",
        "    parts = str(name).split()\n",
        "    return parts[1] if len(parts) > 1 else ''\n",
        "\n",
        "def compare_species(manual, auto):\n",
        "    \"\"\"Сравнение видовых эпитетов с нормализацией\"\"\"\n",
        "    manual_species = extract_species(manual)\n",
        "    auto_species = extract_species(auto)\n",
        "\n",
        "    if not manual_species or not auto_species:\n",
        "        return 0\n",
        "\n",
        "    # Удаляем окончания (us, a, um и т.д.)\n",
        "    manual_stem = re.sub(r'(us|a|um|is|es|i|ae|orum)$', '', normalize_name(manual_species))\n",
        "    auto_stem = re.sub(r'(us|a|um|is|es|i|ae|orum)$', '', normalize_name(auto_species))\n",
        "\n",
        "    return 1 if (\n",
        "        manual_stem == auto_stem or\n",
        "        manual_stem in auto_stem or\n",
        "        auto_stem in manual_stem or\n",
        "        SequenceMatcher(None, manual_stem, auto_stem).ratio() > 0.85\n",
        "    ) else 0\n",
        "\n",
        "def main():\n",
        "    # Загрузка данных\n",
        "    print(\"Загрузка данных...\")\n",
        "    df = pd.read_csv(INPUT_PATH)\n",
        "\n",
        "    # Уникальные образцы с ручной идентификацией\n",
        "    manual_samples = df[df['NameBasedOnSequence'].notna()].drop_duplicates('catNumber')\n",
        "    print(f\"Уникальных образцов с ручной ID: {len(manual_samples)}\")\n",
        "\n",
        "    # Сравнение только для уникальных catNumber\n",
        "    comparison_results = {}\n",
        "    for _, row in tqdm(manual_samples.iterrows(), total=len(manual_samples), desc=\"Сравнение\"):\n",
        "        manual_id = row['NameBasedOnSequence']\n",
        "        auto_id = row['Taxon name'] if not pd.isna(row['Taxon name']) else ''\n",
        "        comparison_results[row['catNumber']] = compare_species(manual_id, auto_id)\n",
        "\n",
        "    # Добавляем результаты в исходный DataFrame\n",
        "    df['identification_match'] = df['catNumber'].map(comparison_results)\n",
        "\n",
        "    # Сохраняем\n",
        "    df.to_csv(OUTPUT_PATH, index=False)\n",
        "    print(f\"\\nРезультаты сохранены в: {OUTPUT_PATH}\")\n",
        "\n",
        "    # Статистика по УНИКАЛЬНЫМ образцам\n",
        "    total = len(comparison_results)\n",
        "    matches = sum(comparison_results.values())\n",
        "\n",
        "    print(\"\\nФинальная статистика (по уникальным образцам):\")\n",
        "    print(f\"Всего образцов: {total}\")\n",
        "    print(f\"Совпадений: {matches} ({matches/total:.1%})\")\n",
        "    print(f\"Расхождений: {total - matches} ({(total - matches)/total:.1%})\")\n",
        "\n",
        "    # Примеры расхождений\n",
        "    discrepancies = [k for k, v in comparison_results.items() if v == 0]\n",
        "    if discrepancies:\n",
        "        print(\"\\nПримеры расхождений (первые 3):\")\n",
        "        for cat in discrepancies[:3]:\n",
        "            sample = manual_samples[manual_samples['catNumber'] == cat].iloc[0]\n",
        "            print(f\"catNumber: {cat}\")\n",
        "            print(f\"Ручная ID: {sample['NameBasedOnSequence']}\")\n",
        "            print(f\"Авто ID: {sample['Taxon name']}\")\n",
        "            print(f\"Сравнивались: '{extract_species(sample['NameBasedOnSequence'])}' vs '{extract_species(sample['Taxon name'])}'\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "WDzfG6WmDFkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Другие анализы"
      ],
      "metadata": {
        "id": "O8jZ2mZtUyOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# загрузка ID номеров из базы NCBI для выгрузки в ENA\n",
        "\n",
        "from Bio import Entrez\n",
        "import pandas as pd\n",
        "import time  # Добавлен импорт модуля time\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# --- Настройки ---\n",
        "Entrez.email = \"filippova.courlee.nina@gmail.com\"  # Обязательно замените на реальный email!\n",
        "API_DELAY = 0.34  # Задержка между запросами (в секундах)\n",
        "\n",
        "# --- Проверка таблицы ---\n",
        "if not isinstance(taxa, pd.DataFrame):\n",
        "    raise ValueError(\"'taxa' должен быть pandas DataFrame!\")\n",
        "if 'name' not in taxa.columns:\n",
        "    raise ValueError(\"Таблица 'taxa' должна содержать колонку 'name'!\")\n",
        "\n",
        "# --- Функция для запроса tax_id с обработкой ошибок ---\n",
        "def get_taxid(species_name):\n",
        "    try:\n",
        "        handle = Entrez.esearch(db=\"taxonomy\", term=species_name, retmode=\"xml\")\n",
        "        record = Entrez.read(handle)\n",
        "        handle.close()\n",
        "        return record[\"IdList\"][0] if record[\"IdList\"] else pd.NA\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка для '{species_name}': {str(e)}\")\n",
        "        return pd.NA\n",
        "\n",
        "# --- Прогресс-бар и обработка ---\n",
        "tqdm.pandas(desc=\"Запрос tax_id из NCBI\")\n",
        "\n",
        "# Применяем функцию с задержкой\n",
        "taxa['tax_id'] = taxa['name'].progress_apply(\n",
        "    lambda x: (time.sleep(API_DELAY), get_taxid(x))[1]\n",
        ")\n",
        "\n",
        "# --- Результаты ---\n",
        "print(\"\\nРезультаты:\")\n",
        "print(f\"Найдено tax_id: {taxa['tax_id'].notna().sum()}/{len(taxa)}\")\n",
        "display(taxa.head())\n",
        "\n",
        "# --- Сохранение ---\n",
        "output_filename = \"taxa_with_taxids.csv\"\n",
        "taxa.to_csv(output_filename, index=False)\n",
        "print(f\"\\nРезультаты сохранены в файл: {output_filename}\")\n",
        "\n",
        "# Для Google Colab (если нужно скачать)\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(output_filename)\n",
        "except ImportError:\n",
        "    pass"
      ],
      "metadata": {
        "id": "XfY0hAiq7m7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# небольшой географический анализ\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.cluster import DBSCAN\n",
        "from geopy.distance import great_circle\n",
        "\n",
        "# Загрузка данных\n",
        "# Укажите правильный путь к вашей рабочей директории\n",
        "work_dir = '/content/drive/MyDrive/Colab Notebooks/IT_expedition/CortDB/'  # Для Google Colab обычно /content/\n",
        "file_name = \"geography1.csv\"  # Убедитесь, что имя файла правильное (возможно опечатка \"gepgraphy\" вместо \"geography\")\n",
        "\n",
        "# Полный путь к файлу\n",
        "file_path = os.path.join(work_dir, file_name)\n",
        "\n",
        "# Проверка существования файла\n",
        "if not os.path.exists(file_path):\n",
        "    raise FileNotFoundError(f\"Файл не найден: {file_path}. Проверьте путь и имя файла.\")\n",
        "\n",
        "# Чтение данных\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Проверка наличия нужных колонок\n",
        "required_columns = ['decimalLatitude', 'decimalLongitude', 'sample_id']\n",
        "for col in required_columns:\n",
        "    if col not in df.columns:\n",
        "        raise ValueError(f\"В данных отсутствует обязательная колонка: {col}\")\n",
        "\n",
        "# Преобразуем координаты в радианы для вычислений\n",
        "coords = np.radians(df[['decimalLatitude', 'decimalLongitude']].values)\n",
        "\n",
        "# Параметры для DBSCAN\n",
        "epsilon = 5 / 6371.0088  # 5 км в радианах (6371.0088 - радиус Земли в км)\n",
        "min_samples = 1  # Минимум 1 точка в кластере\n",
        "\n",
        "# Создаем модель DBSCAN\n",
        "db = DBSCAN(eps=epsilon, min_samples=min_samples, metric='haversine', algorithm='ball_tree')\n",
        "labels = db.fit_predict(coords)\n",
        "\n",
        "# Добавляем метки кластеров в DataFrame\n",
        "df['cluster'] = labels\n",
        "\n",
        "# Функция для нахождения центра кластера\n",
        "def get_center(cluster_df):\n",
        "    return (cluster_df['decimalLatitude'].mean(), cluster_df['decimalLongitude'].mean())\n",
        "\n",
        "# Создаем DataFrame с информацией о кластерах\n",
        "clusters_info = []\n",
        "for cluster_id in df['cluster'].unique():\n",
        "    if cluster_id != -1:  # Игнорируем шум (если есть)\n",
        "        cluster_df = df[df['cluster'] == cluster_id]\n",
        "        center = get_center(cluster_df)\n",
        "        clusters_info.append({\n",
        "            'cluster_id': cluster_id,\n",
        "            'center_latitude': center[0],\n",
        "            'center_longitude': center[1],\n",
        "            'num_samples': len(cluster_df),\n",
        "            'sample_ids': ', '.join(map(str, cluster_df['sample_id'].tolist()))\n",
        "        })\n",
        "\n",
        "clusters_df = pd.DataFrame(clusters_info)\n",
        "\n",
        "# Выводим результаты\n",
        "print(f\"Всего кластеров: {len(clusters_df)}\")\n",
        "print(f\"Всего образцов: {len(df)}\")\n",
        "print(\"\\nИнформация о кластерах:\")\n",
        "print(clusters_df)\n",
        "\n",
        "# Визуализация (опционально)\n",
        "try:\n",
        "    import folium\n",
        "\n",
        "    # Создаем карту с центром в среднем положении всех точек\n",
        "    map_center = [df['decimalLatitude'].mean(), df['decimalLongitude'].mean()]\n",
        "    m = folium.Map(location=map_center, zoom_start=10)\n",
        "\n",
        "    # Добавляем точки на карту\n",
        "    for _, row in df.iterrows():\n",
        "        folium.CircleMarker(\n",
        "            location=[row['decimalLatitude'], row['decimalLongitude']],  # Исправлено: было 'decimalLatitudede'\n",
        "            radius=5,\n",
        "            color='blue' if row['cluster'] != -1 else 'red',\n",
        "            fill=True,\n",
        "            fill_opacity=0.7,\n",
        "            popup=f\"Sample: {row['sample_id']}, Cluster: {row['cluster']}\"\n",
        "        ).add_to(m)\n",
        "\n",
        "    # Добавляем центры кластеров\n",
        "    for _, cluster in clusters_df.iterrows():\n",
        "        folium.Circle(\n",
        "            location=[cluster['center_latitude'], cluster['center_longitude']],\n",
        "            radius=5000,  # 5 км\n",
        "            color='green',\n",
        "            fill=True,\n",
        "            fill_opacity=0.2,\n",
        "            popup=f\"Cluster {cluster['cluster_id']}: {cluster['num_samples']} samples\"\n",
        "        ).add_to(m)\n",
        "\n",
        "    # Сохраняем карту\n",
        "    m.save('clusters_map.html')\n",
        "    print(\"\\nКарта сохранена как 'clusters_map.html'\")\n",
        "except ImportError:\n",
        "    print(\"\\nДля визуализации установите folium: !pip install folium\")\n",
        "\n",
        "# Сохраняем результаты\n",
        "df.to_csv('samples_with_clusters.csv', index=False)\n",
        "clusters_df.to_csv('clusters_info.csv', index=False)"
      ],
      "metadata": {
        "id": "aBgcRUgkD0Cu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}